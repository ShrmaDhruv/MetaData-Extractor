
---------PAGE 1--------


[TITLE] Solving Spatial Supersensing Without Spatial Supersensing


[PLAIN_TEXT] Vishaal Udandarao! Shyamgopal Karthikt SurabhiS. Nath? Andreas Hochlehnert Matthias Bethge! Ameya Prabhu!

[PLAIN_TEXT] 'Tiibingen AI Center, University of Tiibingen ?Max Planck Institute for Biological Cybernetics

[TITLE] ©) github.com/bethgelab/supersanity


[TITLE] Abstract


[PLAIN_TEXT] Cambrian-S aims to take the first steps towards improving video “world models” with spatial supersensing by introducing (i) two benchmarks, VSI-Super-Recall (VSR) and VSI-Super-Counting (VSC), and (ii) bespoke predictive sensing inference strategies tailored to each benchmark. In this work, we conduct a critical analysis of Cambrian-S across both these fronts. First, we introduce a simple baseline, NoSense, which discards almost all temporal structure and uses only a bag-of-words SigLIP model, yet near-perfectly solves VSR, achieving 95% accuracy even on 4-hour videos. This shows benchmarks like VSR can be nearly solved without spatial cognition, world modeling or spatial supersensing. Second, we hypothesize that the tailored inference methods proposed by Cambrian-S likely exploit shortcut heuristics in the benchmark. We illustrate this with a simple sanity check on the VSC benchmark, called VSC-Repeat: We concatenate each video with itself 1-5 times, which does not change the number of unique objects. However, this simple perturbation entirely collapses the mean relative accuracy of Cambrian-S from 42.0% to 0%. A system that performs spatial supersensing and integrates information across experiences should recognize views of the same scene and keep object-count predictions unchanged; instead, Cambrian-S’ inference algorithm relies largely on a shortcut in the VSC benchmark that rooms are never revisited. Taken together, our findings suggest that (i) current VSI-Super benchmarks do not yet reliably measure spatial supersensing, and (ii) predictive-sensing inference recipes used by Cambrian-S improve performance by inadvertently exploiting shortcuts rather than from robust spatial supersensing. We include the response from the Cambrian-S authors (in Appendix A) to provide a balanced perspective alongside our claims.

[IMAGE CAPTION] Figure 1: (Left) NoSense solves VSR without supersensing. Our NoSense baseline uses only a SigLIP model with independent frame-level processing — no video model, LLM, long-term memory, or temporal reasoning — yet almost perfectly solves VSR, showing that the VSR benchmark can be solved without spatial supersensing. (Right) Cambrian-S exploits VSC-specific shortcuts. For the VSC benchmark, we repeat each 10-min video 1—5 times; a supersensing model should output the same object counts, since the unique object count stays the same. Instead, Cambrian-S’ mean relative accuracy collapses from 42% to 0% after 5 repeats, indicating that its surprise-based segmentation inference method relies on VSC-specific shortcuts rather than genuine spatial cognition.


[IMAGE] ./data/extracted_figures/figure_8.png


[IMAGE] ./data/extracted_figures/figure_12.png

